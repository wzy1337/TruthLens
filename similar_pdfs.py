import nltk
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import networkx as nx
import pandas as pd
import re
import string

stop_words = set(nltk.corpus.stopwords.words('english'))
punctuation = set(string.punctuation).union({'`', '-', '``', '–', '“', '’', '”', '•'})

nltk.data.path.append(r"C:\Users\andante\AppData\Roaming\nltk_data")

dataset_path = "Dataset/wikileaks_parsed.csv"

def main(dataset_path, pdf_col_name, summarized_col_name, num_rows=30, threshold=0.3, output_file='similar.jpg'):
    """
    Expects a dataset in CSV format with the following columns. The names of the columns are inputs to the function:
    - pdf_col_name: The file path to the PDF document.
    - summarized_col_name: The summarized text content of the PDF, generated by a ChatGPT or another source.

    Optional Argument:
    - num_rows: Specifies the number of rows to process from the dataset. 30 rows processed by default
    - theshold: Specifies the Similarity threshold required. Default is 0.3
    """

    df = pd.read_csv(dataset_path)

    df_combined = df.groupby(pdf_col_name).agg({
        pdf_col_name: 'first',  # Keep first PDF Path
        summarized_col_name: ' '.join      # Join texts together
    }).reset_index(drop=True)

    # df_combined['PDF_Number'] = df_combined[pdf_col_name].apply(extract_number)
    # df_combined = df_combined.sort_values(by='PDF_Number')  
    df_combined[summarized_col_name] = df_combined[summarized_col_name].apply(preprocess_text)

    df_text = df_combined.head(n=num_rows)[summarized_col_name]
    print(df_combined.head())

    # Create a TF-IDF Vectorizer
    vectorizer = TfidfVectorizer(stop_words='english')

    # Fit and transform the text data
    tfidf_matrix = vectorizer.fit_transform(df_text)

    # Compute cosine similarity between all pairs of documents
    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
    G = nx.Graph()

    for i in range(len(df_text)):
        for j in range(i + 1, len(df_text)):
            similarity = cosine_sim_matrix[i, j]

            if similarity > threshold:
                G.add_edge(
                    df_combined[pdf_col_name].iloc[i], 
                    df_combined[pdf_col_name].iloc[j], 
                    weight=similarity
                )

    # Draw the graph
    pos = nx.spring_layout(G, k=0.5, iterations=15)
    edges = G.edges(data=True)

    weights = [edge[2]['weight'] * 1 for edge in edges]  
    weights = renormalize(weights)

    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=15, font_size=10, font_color='green', width=weights)
    plt.savefig(output_file, bbox_inches="tight")


def renormalize(probs):
    """
    From a list of probabilities, scale them accordingly
    """

    prob_factor = (5 / sum(probs))**0.1**0.2
    return [prob_factor * p for p in probs]

def extract_number(pdf_path):
    """
    Extract the PDF number to sort PDFs from name i.e 1 from 1.pdf
    """
    match = re.search(r'\d+', pdf_path)
    return int(match.group()) if match else float('inf')

def lower_tt(text):
    """
    Returns an uncapitalised version of a text if it is written in Title Case
    """
    words = text.split()
    return ' '.join(map(lambda word: word.lower() if word.istitle() else word, words))

def preprocess_text(text):
    """
    Converts a text <str> into tokens, and removes the tokens which corresponds to puncutations 
    or stop words

    It then returns the uncapitalised version of the text if written in Title Case
    """
    tokens = nltk.tokenize.word_tokenize(text)
    
    # Remove stopwords and punctuation, and convert to lowercase
    tokens = [lower_tt(word) for word in tokens if word.lower() not in stop_words and word not in punctuation]
    
    return ' '.join(tokens)

if __name__ == "__main__":
    main(dataset_path="Dataset/news_excerpts_parsed.csv",
         pdf_col_name="Link",
         summarized_col_name="Text",
         output_file="somefile.jpg",
         threshold=0.08)