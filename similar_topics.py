import networkx as nx
from keybert import KeyBERT
import matplotlib.pyplot as plt
import numpy as np
import streamlit as st
import pandas as pd
import re
from pyvis.network import Network
import streamlit.components.v1 as components
from datetime import timedelta
from urllib.parse import urlparse


def extract_topics(text, kw_model):

    # To diversify the results, we use Maximal Margin Relevance (MMR) and diversity of 0.5 
    topics = kw_model.extract_keywords(
        docs=text, 
        keyphrase_ngram_range=(1,1),
        use_mmr=True,
        diversity=0.5
    )

    # returns a list of [topics of (word, importance)] 
    return topics

def extract_title_from_url(url):
    """
    Extract the Title from a given News URL
    """
    to_ignore = ['index.html', '']

    parsed_url = urlparse(url)
    path_segments = parsed_url.path.split('/')

    filtered_segments = [segment for segment in path_segments]
    
    subsegments = []
    for seg in filtered_segments:
        subsegments.extend(seg.split("-"))

    # Filter out empty segments and segments that are numbers
    filtered_segments = [segment for segment in subsegments if not segment.isdigit() and segment not in to_ignore]

    # Join the segments with spaces and capitalize each word
    title = ' '.join(filtered_segments).title()
    return title

@st.cache_data(persist=True)
def draw_summary_graph(dataframe, start_row=1, end_row=50, threshold=0.3):
    """
    Draw graph after extracting keywords from summary
    """
    topic_graph = nx.Graph()
    kw_model = KeyBERT()

    dataframe['Topics'] = dataframe['Text'].apply(lambda text: extract_topics(text, kw_model))

    for index, row in dataframe[start_row:end_row].iterrows():
        if 'PDF Path' in row.keys():
            pdf_name = row['PDF Path']
        else: # use link identifier
            pdf_name = extract_title_from_url(row['Link'])
        
        topic_graph.add_node(pdf_name, group=10, size=20, shape='box')

        topics = row['Topics']
        
        for topic in topics:
            word, importance = topic

            if importance > threshold:
                topic_graph.add_node(word, group=20)

                topic_graph.add_edge(
                    pdf_name, 
                    word, 
                    weight=importance
                )
    
    dataframe['Topics'] = dataframe['Topics'].apply(lambda li: [topic[0] for topic in li])

    dataframe.to_pickle("./INTERMEDIATE_DF.pkl")
    print("PICKLED DATA")
    return topic_graph

def extract_number(pdf_path):
    """
    Extract the PDF number to sort PDFs from name i.e 1 from 1.pdf
    """
    match = re.search(r'\d+', pdf_path)
    return int(match.group()) if match else float('inf')


def csv_to_dataframe(dataset_path, identifier_colname, summarized_text_colname):
    """
    Expects a dataset given by <dataset_path> in CSV format with the following columns. 
    
    The names of the columns are inputs to the function:
    - identifier_col: The file path to the PDF document.
    - summarized_text_colname: The summarized text content of the PDF, generated by a ChatGPT or another source.
    
    """

    df = pd.read_csv(dataset_path)

    # combine summarized text for those with a common identifier 
    # (i.e two rows in the dataframe with 2.pdf as their identifier)

    df_combined = df.groupby(identifier_colname).agg({
        identifier_colname: 'first',  # Keep first PDF Path
        summarized_text_colname: ' '.join      # Join texts together
    }).reset_index(drop=True)

    # sort all columns by their unique ID
    df_combined['ID'] = df_combined[identifier_colname].apply(extract_number)
    df_combined = df_combined.sort_values(by='ID')

    return df_combined

